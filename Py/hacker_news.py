"""
Hacker News æ—©æŠ¥ï¼ˆè¯„è®ºæ‘˜è¦ç‰ˆï¼‰
name: Hacker Newsæ—©æŠ¥
cron: 30 8 * * *

åŠŸèƒ½è¯´æ˜ï¼š
- è·å– Hacker News çƒ­é—¨æ•…äº‹
- æå–å¹¶æ€»ç»“ç¤¾åŒºè¯„è®º
- ç”Ÿæˆç®€æ´çš„æ—©æŠ¥æ ¼å¼
- åªæ˜¾ç¤ºè¯„è®ºæ‘˜è¦å’Œè®¨è®ºé“¾æ¥
"""

import asyncio
import re
import time
import random
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import List, Optional, Tuple, Dict, Any
import utils.pyEnv as env
from openai import OpenAI
import httpx
from loguru import logger

AI_API_KEY = env.get_env("AI_API_KEY")[0]
AI_BASE_URL = env.get_env("AI_BASE_URL")[0]

logger.remove()
logger.add(
    lambda m: print(m, end=""),
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {message}",
    level="INFO",
)

TZ_LOCAL = datetime.now().astimezone().tzinfo

HN_TOP_STORIES_URL = "https://hacker-news.firebaseio.com/v0/topstories.json"
HN_NEW_STORIES_URL = "https://hacker-news.firebaseio.com/v0/newstories.json"
HN_ITEM_URL = "https://hacker-news.firebaseio.com/v0/item/{}.json"

# Hacker News API é…ç½®
# ä½¿ç”¨å®˜æ–¹ Firebase APIï¼Œæ— éœ€è®¤è¯


@dataclass
class HNComment:
    """Hacker News è¯„è®ºæ•°æ®ç»“æ„"""
    id: int
    text: str
    by: str
    time: datetime
    parent: int
    kids: Optional[List[int]] = None


@dataclass
class HNStory:
    """Hacker News æ•…äº‹æ•°æ®ç»“æ„"""
    id: int
    title: str
    url: Optional[str]
    hn_url: str
    score: int
    by: str
    time: datetime
    descendants: int
    text: Optional[str] = None
    category: str = "general"
    comments: Optional[List[HNComment]] = None  # è¯„è®ºåˆ—è¡¨
    comment_summary: Optional[str] = None  # AI æ€»ç»“çš„è¯„è®ºæ‘˜è¦


def _get_story_category(title: str, url: Optional[str]) -> str:
    """è·å–æ•…äº‹åˆ†ç±»ï¼ˆç®€åŒ–ç‰ˆï¼Œç»Ÿä¸€è¿”å› generalï¼‰"""
    return "general"


# å†…å­˜ç¼“å­˜ï¼Œé¿å…é‡å¤è¯·æ±‚
_story_cache: Dict[int, Optional[HNStory]] = {}
_comment_cache: Dict[int, Optional[HNComment]] = {}


async def fetch_comment_details(
    client: httpx.AsyncClient, cid: int
) -> Optional[HNComment]:
    """è·å–å•ä¸ªè¯„è®ºçš„è¯¦ç»†ä¿¡æ¯"""
    # æ£€æŸ¥ç¼“å­˜
    if cid in _comment_cache:
        logger.debug(f"ä»ç¼“å­˜è·å–è¯„è®ºï¼šID={cid}")
        return _comment_cache[cid]

    try:
        logger.debug(f"æ­£åœ¨è·å–è¯„è®ºï¼šID={cid}")
        r = await client.get(HN_ITEM_URL.format(cid), timeout=10)
        r.raise_for_status()
        data = r.json()
        
        if not data or data.get("type") != "comment":
            logger.warning(f"è¯„è®º {cid} æ•°æ®æ— æ•ˆæˆ–ç±»å‹é”™è¯¯")
            _comment_cache[cid] = None
            return None

        comment = HNComment(
            id=cid,
            text=data.get("text", ""),
            by=data.get("by", ""),
            time=datetime.fromtimestamp(data.get("time", 0), tz=TZ_LOCAL),
            parent=data.get("parent", 0),
            kids=data.get("kids"),
        )
        _comment_cache[cid] = comment
        logger.debug(f"æˆåŠŸè·å–è¯„è®ºï¼šID={cid}, ä½œè€…={comment.by}")
        return comment
    except Exception as e:
        logger.warning(f"è·å–è¯„è®º {cid} å¤±è´¥: {e}")
        _comment_cache[cid] = None
        return None


async def fetch_comments_for_story(
    client: httpx.AsyncClient,
    story_id: int,
    comment_ids: List[int],
    max_comments: int = 10,
) -> List[HNComment]:
    """è·å–æ•…äº‹çš„è¯„è®ºåˆ—è¡¨ï¼ŒåŒ…æ‹¬éƒ¨åˆ†å­è¯„è®º"""
    comments: List[HNComment] = []
    if not comment_ids:
        logger.info(f"æ•…äº‹ {story_id} æ— è¯„è®º")
        return comments

    # é™åˆ¶è·å–çš„è¯„è®ºæ•°é‡ï¼Œé¿å…è¿‡å¤šè¯·æ±‚
    limited_ids = comment_ids[:max_comments]
    logger.info(f"æ•…äº‹ {story_id} å¼€å§‹è·å– {len(limited_ids)} æ¡è¯„è®º")

    sem = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)

    async def fetch(cid):
        async with sem:
            await asyncio.sleep(random.uniform(0.1, 0.3))  # é¿å…è¯·æ±‚è¿‡å¿«
            return await fetch_comment_details(client, cid)

    tasks = [fetch(cid) for cid in limited_ids]
    results = await asyncio.gather(*tasks)

    for res in results:
        if isinstance(res, HNComment):
            comments.append(res)
            # å¦‚æœæœ‰å­è¯„è®ºï¼Œä¹Ÿè·å–ä¸€éƒ¨åˆ†
            if res.kids and len(comments) < max_comments:
                child_comments = await fetch_comments_for_story(
                    client, story_id, res.kids, max_comments - len(comments)
                )
                comments.extend(child_comments)

    logger.info(f"æ•…äº‹ {story_id} æˆåŠŸè·å– {len(comments)} æ¡è¯„è®º")
    return comments[:max_comments]


async def fetch_story_details(client: httpx.AsyncClient, sid: int) -> Optional[HNStory]:
    """è·å–å•ä¸ªæ•…äº‹çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬è¯„è®º"""
    # æ£€æŸ¥ç¼“å­˜
    if sid in _story_cache:
        logger.debug(f"ä»ç¼“å­˜è·å–æ•…äº‹ï¼šID={sid}")
        return _story_cache[sid]

    try:
        logger.debug(f"æ­£åœ¨è·å–æ•…äº‹ï¼šID={sid}")
        r = await client.get(HN_ITEM_URL.format(sid), timeout=10)
        r.raise_for_status()
        data = r.json()
        
        if not data or data.get("type") != "story":
            logger.warning(f"æ•…äº‹ {sid} æ•°æ®æ— æ•ˆæˆ–ç±»å‹é”™è¯¯")
            _story_cache[sid] = None
            return None

        story = HNStory(
            id=sid,
            title=data.get("title", ""),
            url=data.get("url"),
            hn_url=f"https://news.ycombinator.com/item?id={sid}",
            score=data.get("score", 0),
            by=data.get("by", ""),
            time=datetime.fromtimestamp(data.get("time", 0), tz=TZ_LOCAL),
            descendants=data.get("descendants", 0),
            text=data.get("text"),
            category=_get_story_category(data.get("title", ""), data.get("url")),
            comments=None,
            comment_summary=None,
        )

        # è·å–è¯„è®º
        if data.get("kids") and len(data.get("kids", [])) > 0:
            logger.info(f"æ•…äº‹ {sid} æœ‰ {len(data.get('kids', []))} æ¡è¯„è®ºï¼Œå¼€å§‹è·å–")
            story.comments = await fetch_comments_for_story(
                client, sid, data.get("kids", []), max_comments=10
            )
        else:
            logger.info(f"æ•…äº‹ {sid} æ— è¯„è®º")

        _story_cache[sid] = story
        logger.info(f"æˆåŠŸè·å–æ•…äº‹ï¼šID={sid}, æ ‡é¢˜={story.title[:50]}...")
        return story
    except Exception as e:
        logger.warning(f"è·å–æ•…äº‹ {sid} å¤±è´¥: {e}")
        _story_cache[sid] = None
        return None


async def fetch_story_ids(story_type: str = "top") -> List[int]:
    """è·å–æ•…äº‹ ID åˆ—è¡¨"""
    url = {"top": HN_TOP_STORIES_URL, "new": HN_NEW_STORIES_URL}[story_type]
    logger.info(f"æ­£åœ¨è·å– {story_type} æ•…äº‹åˆ—è¡¨...")
    
    async with httpx.AsyncClient() as c:
        r = await c.get(url, timeout=10)
        r.raise_for_status()
        ids = r.json()
        logger.info(f"æˆåŠŸè·å– {len(ids)} ä¸ªæ•…äº‹ ID")
        return ids


async def collect_recent_stories_async(
    hours: int, max_stories: int, story_type: str = "top", min_score: int = 10
) -> List[HNStory]:
    """
    æ”¶é›†æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„çƒ­é—¨æ•…äº‹
    """
    start = time.time()
    cutoff = datetime.now(TZ_LOCAL) - timedelta(hours=hours)
    logger.info(f"æ—¶é—´ç­›é€‰ï¼šè·å– {hours} å°æ—¶å†…çš„æ•…äº‹ï¼ˆæˆªæ­¢æ—¶é—´ï¼š{cutoff:%F %T}ï¼‰")
    logger.info(f"æœ€ä½åˆ†æ•°è¦æ±‚ï¼š{min_score}")

    ids = await fetch_story_ids(story_type)
    if not ids:
        logger.warning("æœªè·å–åˆ°ä»»ä½•æ•…äº‹ ID")
        return []

    # å¹¶å‘æ‹‰å–ï¼Œæ§åˆ¶å¹¶å‘æ•°é‡
    sem = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)

    async def fetch(sid):
        async with sem:
            await asyncio.sleep(random.uniform(0.1, 0.3))  # é¿å…è¯·æ±‚è¿‡å¿«
            return await fetch_story_details(session, sid)

    stories: List[HNStory] = []
    async with httpx.AsyncClient() as session:
        for i in range(0, len(ids), 50):  # æ¯æ‰¹å¤„ç† 50 ä¸ª
            batch = ids[i : i + 50]
            logger.info(f"å¤„ç†æ‰¹æ¬¡ {i//50 + 1}ï¼šæ•…äº‹ ID {batch[0]} - {batch[-1]}")
            
            tasks = [fetch(sid) for sid in batch]
            for res in await asyncio.gather(*tasks):
                if (
                    isinstance(res, HNStory)
                    and res.time >= cutoff
                    and res.score >= min_score
                ):
                    stories.append(res)
                    logger.info(f"ç¬¦åˆæ¡ä»¶çš„æ•…äº‹ï¼š{res.title[:50]}... (åˆ†æ•°: {res.score})")
            
            if len(stories) >= max_stories:
                logger.info(f"å·²æ”¶é›†åˆ° {len(stories)} ä¸ªæ•…äº‹ï¼Œè¾¾åˆ°ç›®æ ‡æ•°é‡")
                break
            await asyncio.sleep(random.uniform(0.5, 1.0))  # æ‰¹æ¬¡é—´å»¶è¿Ÿ

    stories.sort(key=lambda s: s.score, reverse=True)
    final_stories = stories[:max_stories]
    logger.info(f"æ”¶é›†å®Œæˆï¼š{len(final_stories)} ä¸ªæ•…äº‹ï¼Œè€—æ—¶ {time.time()-start:.1f} ç§’")
    return final_stories


def openai_client(api_key=None, base_url=None) -> Optional[OpenAI]:
    """åˆ›å»º OpenAI å®¢æˆ·ç«¯"""
    key = api_key or AI_API_KEY
    url = base_url or AI_BASE_URL
    if not key or len(key) < 10:
        logger.warning("API Key æ— æ•ˆæˆ–è¿‡çŸ­")
        return None
    try:
        logger.info("æ­£åœ¨åˆ›å»º OpenAI å®¢æˆ·ç«¯...")
        client = OpenAI(api_key=key, base_url=url)
        logger.info("OpenAI å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
        return client
    except Exception as e:
        logger.error(f"åˆ›å»º OpenAI å®¢æˆ·ç«¯å¤±è´¥: {e}")
        return None


def generate_local_summary(story: HNStory) -> str:
    """ç”Ÿæˆæœ¬åœ°æ‘˜è¦ï¼ˆå…œåº•æ–¹æ¡ˆï¼‰"""
    return story.title if len(story.title) <= 60 else story.title[:57] + "..."

def ai_summarize_comments(client: OpenAI, model: str, story: HNStory) -> str:
    """ä½¿ç”¨ AI æ€»ç»“æ•…äº‹çš„è¯„è®º"""
    if not story.comments or len(story.comments) == 0:
        logger.info(f"æ•…äº‹ {story.id} æ— è¯„è®ºï¼Œè·³è¿‡æ€»ç»“")
        return "æš‚æ— æœ‰ä»·å€¼çš„è¯„è®º"

    logger.info(f"å¼€å§‹æ€»ç»“æ•…äº‹ {story.id} çš„ {len(story.comments)} æ¡è¯„è®º")

    # æå–è¯„è®ºæ–‡æœ¬ï¼ˆå»é™¤HTMLæ ‡ç­¾ï¼‰
    comment_texts = []
    for comment in story.comments:
        if comment.text:
            # ç®€å•å»é™¤HTMLæ ‡ç­¾
            clean_text = re.sub(r"<.*?>", "", comment.text)
            comment_texts.append(f"ç”¨æˆ·{comment.by}ï¼š{clean_text}")

    if not comment_texts:
        logger.warning(f"æ•…äº‹ {story.id} è¯„è®ºæ–‡æœ¬ä¸ºç©º")
        return "æš‚æ— æœ‰ä»·å€¼çš„è¯„è®º"

    system = "ä½ æ˜¯ä¸“ä¸šè¯„è®ºæ‘˜è¦åŠ©æ‰‹ï¼Œè¯·ç”¨ä¸­æ–‡ 30 å­—å·¦å³æ€»ç»“ä»¥ä¸‹HNè¯„è®ºçš„ä¸»è¦è§‚ç‚¹å’Œè®¨è®ºç„¦ç‚¹ï¼Œä¸æ·»åŠ ä¸ªäººè§‚ç‚¹ï¼Œä¸è¾“å‡ºmarkdownï¼Œåªè¦é‡è¦çš„è§‚ç‚¹"
    user = "è¯„è®ºåˆ—è¡¨ï¼š\n" + "\n".join(comment_texts)

    payload = dict(
        model=model,
        messages=[
            {"role": "system", "content": system},
            {"role": "user", "content": user},
        ],
        temperature=0.3,
        top_p=0.8,
    )

    for retry in range(3):
        try:
            logger.debug(f"æ•…äº‹ {story.id} è¯„è®ºæ€»ç»“ç¬¬ {retry+1} æ¬¡å°è¯•")
            resp = client.chat.completions.create(**payload)
            summary = re.sub(r"\s+", " ", resp.choices[0].message.content).strip()
            logger.info(f"æ•…äº‹ {story.id} è¯„è®ºæ€»ç»“æˆåŠŸ")
            return summary
        except Exception as e:
            wait = 2**retry
            logger.warning(f"æ•…äº‹ {story.id} è¯„è®ºæ€»ç»“å¤±è´¥ï¼ˆé‡è¯• {retry+1}/3ï¼‰: {e} -> ç­‰å¾… {wait}s")
            time.sleep(wait)
    
    logger.error(f"æ•…äº‹ {story.id} è¯„è®ºæ€»ç»“å¤šæ¬¡å¤±è´¥")
    return ""


async def batch_ai_summarize_async(
    stories: List[HNStory], api_key: str, model: str
) -> List[HNStory]:
    """æ‰¹é‡ AI æ€»ç»“è¯„è®º"""
    if not stories:
        logger.warning("æ— æ•…äº‹éœ€è¦æ€»ç»“")
        return []
    
    logger.info(f"å¼€å§‹æ‰¹é‡æ€»ç»“ {len(stories)} ä¸ªæ•…äº‹çš„è¯„è®º")
    client = openai_client(api_key)
    if not client:
        logger.error("æ— æ³•åˆ›å»º AI å®¢æˆ·ç«¯ï¼Œè·³è¿‡è¯„è®ºæ€»ç»“")
        return stories
    
    sem = asyncio.Semaphore(MAX_CONCURRENT_AI)

    async def do(story, idx):
        async with sem:
            await asyncio.sleep(random.uniform(0.2, 0.5))  # é¿å…è¯·æ±‚è¿‡å¿«
            loop = asyncio.get_event_loop()

            # åªæ€»ç»“è¯„è®º
            if story.comments and len(story.comments) > 0:
                logger.info(f"æ­£åœ¨æ€»ç»“æ•…äº‹ {idx}/{len(stories)}ï¼š{story.title[:30]}...")
                comment_summary = await loop.run_in_executor(
                    None, ai_summarize_comments, client, model, story
                )
                story.comment_summary = comment_summary
            else:
                logger.info(f"æ•…äº‹ {idx}/{len(stories)} æ— è¯„è®ºï¼Œè·³è¿‡æ€»ç»“")

            return story

    tasks = [do(s, i) for i, s in enumerate(stories, 1)]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    ok = [r for r in results if isinstance(r, HNStory)]
    logger.info(f"è¯„è®ºæ€»ç»“å®Œæˆï¼šæˆåŠŸ {len(ok)}/{len(stories)} ä¸ªæ•…äº‹")
    return ok


def render_markdown_report(
    date_label: str, stories: List[HNStory]
) -> str:
    """ç”Ÿæˆ Markdown æ ¼å¼çš„æ—©æŠ¥"""
    if not stories:
        logger.warning("æ— æ•…äº‹æ•°æ®ï¼Œç”Ÿæˆç©ºæŠ¥å‘Š")
        return f"# Hacker News æ—©æŠ¥ï¼ˆ{date_label}ï¼‰\næš‚æ— å†…å®¹"
    
    logger.info(f"å¼€å§‹ç”Ÿæˆæ—©æŠ¥ï¼ŒåŒ…å« {len(stories)} ä¸ªæ•…äº‹")
    lines = [
        f"# Hacker News æ—©æŠ¥ï¼ˆ{date_label}ï¼‰",
    ]
    
    for i, story in enumerate(stories, 1):
        # æ·»åŠ è¯„è®ºæ€»ç»“
        if story.comment_summary:
            lines.append(f"> {story.comment_summary}")
            logger.debug(f"æ·»åŠ æ•…äº‹ {i} çš„è¯„è®ºæ‘˜è¦")
        else:
            logger.debug(f"æ•…äº‹ {i} æ— è¯„è®ºæ‘˜è¦")
        lines.append(f"è®¨è®ºï¼š{story.hn_url}")
    
    report = "\n".join(lines)
    logger.info(f"æ—©æŠ¥ç”Ÿæˆå®Œæˆï¼Œæ€»é•¿åº¦ï¼š{len(report)} å­—ç¬¦")
    return report


# é…ç½®å‚æ•°
DEFAULT_HOURS = 24  # é»˜è®¤è·å–24å°æ—¶å†…çš„æ•…äº‹
DEFAULT_MAX_STORIES = 10  # é»˜è®¤æœ€å¤š10ä¸ªæ•…äº‹
DEFAULT_MODEL = "glm-4-flash"  # é»˜è®¤AIæ¨¡å‹
DEFAULT_CONCURRENT = True  # é»˜è®¤ä½¿ç”¨å¹¶å‘æ¨¡å¼
MAX_CONCURRENT_REQUESTS = 15  # æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
MAX_CONCURRENT_AI = 20  # AIæ¥å£æœ€å¤§å¹¶å‘æ•°


async def main_async(
    hours: int = DEFAULT_HOURS,
    max_stories: int = DEFAULT_MAX_STORIES,
    model: str = DEFAULT_MODEL,
    story_type: str = "top",
    min_score: int = 10,
):
    """ä¸»å¼‚æ­¥å‡½æ•°"""
    logger.info("=" * 60)
    logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œ Hacker News æ—©æŠ¥ä»»åŠ¡")
    logger.info(f"ğŸ“Š é…ç½®ï¼š{hours}å°æ—¶å†…ï¼Œæœ€å¤š{max_stories}ä¸ªæ•…äº‹ï¼Œæœ€ä½åˆ†æ•°{min_score}")
    logger.info("=" * 60)
    
    # æ­¥éª¤1ï¼šæ”¶é›†æ•…äº‹
    logger.info("ã€æ­¥éª¤1ã€‘å¼€å§‹çˆ¬å– Hacker News æ•…äº‹")
    stories = await collect_recent_stories_async(
        hours, max_stories, story_type, min_score
    )
    if not stories:
        logger.warning("âŒ æœªè·å–åˆ°ä»»ä½•æ•…äº‹ï¼Œä»»åŠ¡ç»“æŸ")
        return

    logger.info(f"âœ… æˆåŠŸè·å– {len(stories)} ä¸ªæ•…äº‹")

    # æ­¥éª¤2ï¼šAI æ€»ç»“è¯„è®º
    logger.info("=" * 60)
    logger.info("ã€æ­¥éª¤2ã€‘å¼€å§‹ AI æ€»ç»“è¯„è®º")
    logger.info("=" * 60)
    summarized_stories = await batch_ai_summarize_async(stories, AI_API_KEY, model)

    # æ­¥éª¤3ï¼šç”ŸæˆæŠ¥å‘Š
    logger.info("=" * 60)
    logger.info("ã€æ­¥éª¤3ã€‘ç”Ÿæˆæ—©æŠ¥")
    logger.info("=" * 60)
    date = datetime.now(TZ_LOCAL).strftime("%F")
    md = render_markdown_report(date, summarized_stories)

    # å‘é€é€šçŸ¥
    logger.info("=" * 60)
    logger.info("ã€æ­¥éª¤4ã€‘å‘é€é€šçŸ¥")
    logger.info("=" * 60)
    try:
        logger.info("ğŸ“¤ æ­£åœ¨å‘é€æ—©æŠ¥é€šçŸ¥...")
        QLAPI.notify("Hacker News æ—©æŠ¥", md)
        logger.info("âœ… æ—©æŠ¥é€šçŸ¥å‘é€æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ å‘é€é€šçŸ¥å¤±è´¥: {e}")
    
    logger.info("ğŸ‰ Hacker News æ—©æŠ¥ä»»åŠ¡å®Œæˆï¼")


def main():
    """ä¸»å‡½æ•°å…¥å£"""
    logger.info("ğŸ¯ å¯åŠ¨ Hacker News æ—©æŠ¥è„šæœ¬")
    try:
        asyncio.run(main_async())
    except Exception as e:
        logger.error(f"ğŸ’¥ è„šæœ¬æ‰§è¡Œå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()